count(word, sort=T) %>%
with(wordcloud(word, n, scale=c(3,0.25), max.words = 1000, random.order=FALSE, color = brewer.pal(8, "Dark2")))
runApp()
ip <- read_html("https://weheartit.com/lucashrossi") %>%
html_nodes("#content") %>%
html_attr("data-infinite-scroll-count")
ip <- as.numeric(ip)
ipp <- 1
for(i in 1:ip) {
pagen <- "https://weheartit.com/lucashrossi"
pagen <- paste(pagen,"?page=",sep="")
pagen <- paste(pagen,ipp,sep="")
if(ipp==1) {
wh <- read_html(pagen) %>%
html_nodes(".entry-thumbnail") %>%
html_attr("src")
wh <- as.data.frame(wh)
colnames(wh)[1] = "durl"
} else {
wh1 <- read_html(pagen) %>%
html_nodes(".entry-thumbnail") %>%
html_attr("src")
#wh <- paste(wh,wh,sep=",")
wh1 <- as.data.frame(wh1)
colnames(wh1)[1] = "durl"
wh <- rbind(wh, wh1)
}
ipp <- ipp +1
}
View(wh)
ii <- 1
iii <- 1
#for(i in seq_along(wh)) {
#for(i in 1:nrow(wh)) {
for(i in 1:5) {
iii <- iii +1
#st <- wh[i]
st <- wh[ii,1]
st <- substring(st, first = 1, last = 40)
st <- paste(st,"/original.jpg",sep="")
destf <- paste("Zip/Imagen",ii,sep="")
destf <- paste(destf,".jpg",sep="")
result <- "Error?"
result = tryCatch({
download.file(url = st, destfile = destf)
}, warning = function(w) {
print(paste("MY_WARNING:  ",w))
w <- ""
}, error = function(e) {
print(paste("MY_ERROR:  ",e))
e <- ""
}, finally = {
print(paste("result =",result))
})
ii <- ii +1
print(st)
print(destf)
print(ii)
print(iii)
}
#for(i in seq_along(wh)) {
#for(i in 1:nrow(wh)) {
for(i in 1:5) {
iii <- iii +1
#st <- wh[i]
st <- wh[ii,1]
st <- substring(st, first = 1, last = 40)
st <- paste(st,"/original.jpg",sep="")
destf <- paste("Zip/Imagen",ii,sep="")
destf <- paste(destf,".jpg",sep="")
result <- "Error?"
result = tryCatch({
download.file(url = st, destfile = destf)
}, warning = function(w) {
print(paste("MY_WARNING:  ",w))
w <- ""
}, error = function(e) {
print(paste("MY_ERROR:  ",e))
e <- ""
}, finally = {
print(paste("result =",result))
})
ii <- ii +1
print(st)
print(destf)
print(ii)
print(iii)
}
ii <- 1
iii <- 1
#for(i in seq_along(wh)) {
#for(i in 1:nrow(wh)) {
for(i in 1:5) {
iii <- iii +1
#st <- wh[i]
st <- wh[ii,1]
st <- substring(st, first = 1, last = 40)
st <- paste(st,"/original.jpg",sep="")
destf <- paste("Zip/Imagen",ii,sep="")
destf <- paste(destf,".jpg",sep="")
result <- "Error?"
result = tryCatch({
download.file(url = st, destfile = destf)
}, warning = function(w) {
print(paste("MY_WARNING:  ",w))
w <- ""
}, error = function(e) {
print(paste("MY_ERROR:  ",e))
e <- ""
}, finally = {
print(paste("result =",result))
})
ii <- ii +1
print(st)
print(destf)
print(ii)
print(iii)
}
#scraping_wiki <- read_html("https://weheartit.com/")
scraping_wiki <- read_html("https://www.clarin.com/")
all_text <- scraping_wiki %>%
html_nodes("div") %>%
html_text() %>%
strsplit(split = "\n") %>%
unlist() %>%
.[. != ""]
all_text %>%
str_replace_all(pattern = "\n", replacement = " ") %>%
str_replace_all(pattern = "[\\^]", replacement = " ") %>%
str_replace_all(pattern = "\"", replacement = " ") %>%
str_replace_all(pattern = "\\s+", replacement = " ") %>%
str_trim(side = "both")
all_text <- as.data.frame(all_text)
all_text <- all_text %>%
unnest_tokens(word, all_text)
all_text %>%
anti_join(stop_words) %>%
count(word, sort=T) %>%
with(wordcloud(word, n, scale=c(3,0.25), max.words = 1000, random.order=FALSE, color = brewer.pal(8, "Dark2")))
#scraping_wiki <- read_html("https://weheartit.com/")
scraping_wiki <- read_html("https://www.nytimes.com/")
all_text <- scraping_wiki %>%
html_nodes("div") %>%
html_text() %>%
strsplit(split = "\n") %>%
unlist() %>%
.[. != ""]
all_text %>%
str_replace_all(pattern = "\n", replacement = " ") %>%
str_replace_all(pattern = "[\\^]", replacement = " ") %>%
str_replace_all(pattern = "\"", replacement = " ") %>%
str_replace_all(pattern = "\\s+", replacement = " ") %>%
str_trim(side = "both")
all_text <- as.data.frame(all_text)
all_text <- all_text %>%
unnest_tokens(word, all_text)
all_text %>%
anti_join(stop_words) %>%
count(word, sort=T) %>%
with(wordcloud(word, n, scale=c(3,0.25), max.words = 1000, random.order=FALSE, color = brewer.pal(8, "Dark2")))
tw_toblerone <- search_tweets(
"nytimes", n = 1000, include_rts = TRUE)
nodos <- gather(data = tw_toblerone, key = "tipo", value = "identificacion", c(1,54))
nodos$retweet_count <- as.numeric(nodos$retweet_count)
nodos$favorite_count <- as.numeric(nodos$favorite_count)
which.duplicates<-rownames(nodos[duplicated(nodos$identificacion),])
nodos <- nodos[-c(as.integer(which.duplicates)),]
nodos <- nodos %>% select(identificacion, screen_name, is_retweet, favorite_count, retweet_count, verified)  %>% rename( name = screen_name)
links <- tw_toblerone %>% group_by(user_id, retweet_user_id) %>% summarise(cantidad = n()) %>%
rename(from = user_id,
to = retweet_user_id,
friendship = cantidad)
g <- graph_from_data_frame(links, directed=TRUE, vertices=nodos)
which.duplicates<-rownames(nodos[duplicated(nodos$identificacion),])
nodos <- nodos[-c(as.integer(which.duplicates)),]
nodos <- nodos %>% select(identificacion, screen_name, is_retweet, favorite_count, retweet_count, verified)  %>% rename( name = screen_name)
tw_toblerone <- search_tweets(
"nytimes", n = 1000, include_rts = TRUE)
View(tw_toblerone)
tw_toblerone <- data.frame(lapply(tw_toblerone, as.character), stringsAsFactors=FALSE)
nodos <- gather(data = tw_toblerone, key = "tipo", value = "identificacion", c(1,54))
nodos$retweet_count <- as.numeric(nodos$retweet_count)
nodos$favorite_count <- as.numeric(nodos$favorite_count)
which.duplicates<-rownames(nodos[duplicated(nodos$identificacion),])
nodos <- nodos[-c(as.integer(which.duplicates)),]
nodos <- nodos %>% select(identificacion, screen_name, is_retweet, favorite_count, retweet_count, verified)  %>% rename( name = screen_name)
links <- tw_toblerone %>% group_by(user_id, retweet_user_id) %>% summarise(cantidad = n()) %>%
rename(from = user_id,
to = retweet_user_id,
friendship = cantidad)
g <- graph_from_data_frame(links, directed=TRUE, vertices=nodos)
V(g)$label.cex <- seq(0.01,1.5,length.out=500)
V(g)$size      <- seq(1,15,length.out=500)
plot(g, vertex.label = V(g)$name,
vertex.shape="circle",
vertex.color="red")
gf <- graph_from_data_frame(links, directed=TRUE, vertices=nodos)
gf.outd <- degree(gf, mode = c("out"))
gf.ind <- degree(gf, mode = c("in"))
my.label<- names(gf.ind)
my.label[which(log(gf.ind+1) < 2.1)]<- ""
my.label2<- my.label
l <- layout.fruchterman.reingold(gf, grid = c("nogrid"))
plot(gf, vertex.label = my.label2,  vertex.size = 5, layout = l)
gw <- graph_from_data_frame(links, directed=TRUE, vertices=nodos)
ll <- layout.fruchterman.reingold(gw, grid = c("nogrid"))
wc <- walktrap.community(gw)
users_wc <- membership(wc)
new.color<-data.frame(t(col2rgb(wc$membership)/255))
new.color<-rgb(new.color, alpha=.6)
gw.ind <- degree(gw, mode = c("in"))
plot(wc, gw, vertex.label = NA,  vertex.size=log(gw.ind+1), vertex.color=new.color, edge.size = 0.5, edge.color = "grey",  layout = ll)
tw_toblerone <- search_tweets(
"nytimes", n = 1000, include_rts = TRUE)
tw_toblerone <- data.frame(lapply(tw_toblerone, as.character), stringsAsFactors=FALSE)
write.csv(tw_toblerone, file = "C:/Users/hernan/Desktop/NYT.csv")
#tw_toblerone <- read.csv("WeHaertit.csv")
tw_toblerone <- read.csv("NYT.csv")
drop <- c("X")
tw_toblerone = tw_toblerone[,!(names(tw_toblerone) %in% drop)]
nodos <- gather(data = tw_toblerone, key = "tipo", value = "identificacion", c(1,54))
nodos$retweet_count <- as.numeric(nodos$retweet_count)
nodos$favorite_count <- as.numeric(nodos$favorite_count)
which.duplicates<-rownames(nodos[duplicated(nodos$identificacion),])
nodos <- nodos[-c(as.integer(which.duplicates)),]
links <- tw_toblerone %>% group_by(user_id, retweet_user_id) %>% summarise(cantidad = n()) %>%
rename(from = user_id,
to = retweet_user_id,
friendship = cantidad)
g <- graph_from_data_frame(links, directed=TRUE, vertices=nodos)
View(tw_toblerone)
nodos <- gather(data = tw_toblerone, key = "tipo", value = "identificacion", c(1,54))
nodos$retweet_count <- as.numeric(nodos$retweet_count)
nodos$favorite_count <- as.numeric(nodos$favorite_count)
which.duplicates<-rownames(nodos[duplicated(nodos$identificacion),])
nodos <- nodos[-c(as.integer(which.duplicates)),]
nodos <- nodos %>% select(identificacion, screen_name, is_retweet, favorite_count, retweet_count, verified)  %>% rename( name = screen_name)
links <- tw_toblerone %>% group_by(user_id, retweet_user_id) %>% summarise(cantidad = n()) %>%
rename(from = user_id,
to = retweet_user_id,
friendship = cantidad)
g <- graph_from_data_frame(links, directed=TRUE, vertices=nodos)
plot(g, vertex.label = V(g)$name,
vertex.shape="circle",
vertex.color="red")
V(g)$label.cex <- seq(0.01,1.5,length.out=500)
V(g)$size      <- seq(1,15,length.out=500)
plot(g, vertex.label = V(g)$name,
vertex.shape="circle",
vertex.color="red")
gf <- graph_from_data_frame(links, directed=TRUE, vertices=nodos)
gf.outd <- degree(gf, mode = c("out"))
gf.ind <- degree(gf, mode = c("in"))
my.label<- names(gf.ind)
my.label[which(log(gf.ind+1) < 2.1)]<- ""
my.label2<- my.label
l <- layout.fruchterman.reingold(gf, grid = c("nogrid"))
plot(gf, vertex.label = my.label2,  vertex.size = 5, layout = l)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
all_text %>%
anti_join(stop_words) %>%
count(word, sort=T) %>%
with(wordcloud(word, n, scale=c(2,0.25), max.words = 1000, random.order=FALSE, color = brewer.pal(8, "Dark2")))
#scraping_wiki <- read_html("https://weheartit.com/")
scraping_wiki <- read_html("https://www.nytimes.com/")
all_text <- scraping_wiki %>%
html_nodes("div") %>%
html_text() %>%
strsplit(split = "\n") %>%
unlist() %>%
.[. != ""]
all_text %>%
str_replace_all(pattern = "\n", replacement = " ") %>%
str_replace_all(pattern = "[\\^]", replacement = " ") %>%
str_replace_all(pattern = "\"", replacement = " ") %>%
str_replace_all(pattern = "\\s+", replacement = " ") %>%
str_trim(side = "both")
all_text <- as.data.frame(all_text)
all_text <- all_text %>%
unnest_tokens(word, all_text)
all_text %>%
anti_join(stop_words) %>%
count(word, sort=T) %>%
with(wordcloud(word, n, scale=c(2,0.25), max.words = 1000, random.order=FALSE, color = brewer.pal(8, "Dark2")))
all_text %>%
anti_join(stop_words) %>%
count(word, sort=T) %>%
with(wordcloud(word, n, scale=c(1.5,0.25), max.words = 1000, random.order=FALSE, color = brewer.pal(8, "Dark2")))
g <- graph_from_data_frame(links, directed=TRUE, vertices=nodos)
V(g)$label.cex <- seq(0.01,1.5,length.out=500)
V(g)$size      <- seq(1,15,length.out=500)
plot(g, vertex.label = V(g)$name,
vertex.shape="circle",
vertex.color="red")
V(g)$size      <- seq(1,10,length.out=500)
plot(g, vertex.label = V(g)$name,
vertex.shape="circle",
vertex.color="red")
V(g)$label.cex <- seq(0.01,1,length.out=500)
plot(g, vertex.label = V(g)$name,
vertex.shape="circle",
vertex.color="red")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
iris
Species <- c('https://www.nytimes.com/','https://clarin.com/','https://weheartit.com/')
Tipo <- c("Noticias", "Noticias", "Red Social")
Lugar <- c("Internacional", "Nacional", "Internacional")
shainy.data <- data.frame(Species, Tipo, Lugar)
runApp()
runApp()
Species <- c('https://www.nytimes.com/','https://clarin.com/','https://weheartit.com/')
Tipo <- c("Noticias", "Noticias", "Red Social")
Lugar <- c("Internacional", "Nacional", "Internacional")
shai.data <- data.frame(Species, Tipo, Lugar)
View(shai.data)
iris
shai
shai <- data.frame(Species, Tipo, Lugar)
shai
runApp()
runApp()
runApp()
runApp()
runApp()
View(all_text)
runApp()
View(all_text)
View(links)
runApp()
#scraping_wiki <- read_html("https://weheartit.com/")
scraping_wiki <- read_html("https://www.nytimes.com/")
all_text <- scraping_wiki %>%
html_nodes("div") %>%
html_text() %>%
strsplit(split = "\n") %>%
unlist() %>%
.[. != ""]
all_text %>%
str_replace_all(pattern = "\n", replacement = " ") %>%
str_replace_all(pattern = "[\\^]", replacement = " ") %>%
str_replace_all(pattern = "\"", replacement = " ") %>%
str_replace_all(pattern = "\\s+", replacement = " ") %>%
str_trim(side = "both")
all_text <- as.data.frame(all_text)
all_text <- all_text %>%
unnest_tokens(word, all_text)
View(all_text)
all_text <- all_text$x1 <- 'z'
#scraping_wiki <- read_html("https://weheartit.com/")
scraping_wiki <- read_html("https://www.nytimes.com/")
all_text <- scraping_wiki %>%
html_nodes("div") %>%
html_text() %>%
strsplit(split = "\n") %>%
unlist() %>%
.[. != ""]
all_text %>%
str_replace_all(pattern = "\n", replacement = " ") %>%
str_replace_all(pattern = "[\\^]", replacement = " ") %>%
str_replace_all(pattern = "\"", replacement = " ") %>%
str_replace_all(pattern = "\\s+", replacement = " ") %>%
str_trim(side = "both")
all_text <- as.data.frame(all_text)
all_text <- all_text %>%
unnest_tokens(word, all_text)
all_text1 <- cbind(all_text, x1='z')
View(all_text1)
runApp()
runApp()
runApp()
runApp()
View(shai)
View(all_text)
scraping_wiki <- read_html("https://www.nytimes.com/")
all_text <- scraping_wiki %>%
html_nodes("div") %>%
html_text() %>%
strsplit(split = "\n") %>%
unlist() %>%
.[. != ""]
all_text %>%
str_replace_all(pattern = "\n", replacement = " ") %>%
str_replace_all(pattern = "[\\^]", replacement = " ") %>%
str_replace_all(pattern = "\"", replacement = " ") %>%
str_replace_all(pattern = "\\s+", replacement = " ") %>%
str_trim(side = "both")
all_text <- as.data.frame(all_text)
all_text <- all_text %>%
unnest_tokens(word, all_text)
all_text <- cbind(all_text, Pagina='https://www.nytimes.com/')
all_text1 <- all_text
scraping_wiki <- read_html("https://weheartit.com/")
all_text <- scraping_wiki %>%
html_nodes("div") %>%
html_text() %>%
strsplit(split = "\n") %>%
unlist() %>%
.[. != ""]
all_text %>%
str_replace_all(pattern = "\n", replacement = " ") %>%
str_replace_all(pattern = "[\\^]", replacement = " ") %>%
str_replace_all(pattern = "\"", replacement = " ") %>%
str_replace_all(pattern = "\\s+", replacement = " ") %>%
str_trim(side = "both")
all_text <- as.data.frame(all_text)
all_text <- all_text %>%
unnest_tokens(word, all_text)
all_text <- cbind(all_text, Pagina='https://www.weheartit.com/')
all_text2 <- all_text
scraping_wiki <- read_html("https://clarin.com/")
all_text <- scraping_wiki %>%
html_nodes("div") %>%
html_text() %>%
strsplit(split = "\n") %>%
unlist() %>%
.[. != ""]
all_text %>%
str_replace_all(pattern = "\n", replacement = " ") %>%
str_replace_all(pattern = "[\\^]", replacement = " ") %>%
str_replace_all(pattern = "\"", replacement = " ") %>%
str_replace_all(pattern = "\\s+", replacement = " ") %>%
str_trim(side = "both")
all_text <- as.data.frame(all_text)
all_text <- all_text %>%
unnest_tokens(word, all_text)
all_text <- cbind(all_text, Pagina='https://www.clarin.com/')
all_text3 <- all_text
all_text <- rbind(all_text1, all_text2, all_text3)
View(all_text)
Pagina <- c('https://www.nytimes.com/','https://www.washingtonpost.com/','https://weheartit.com/')
Tipo <- c("Noticias", "Noticias", "Red Social")
Lugar <- c("Internacional", "Internacional", "Internacional")
shai <- data.frame(Species, Tipo, Lugar)
shai <- data.frame(Pagina, Tipo, Lugar)
View(shai)
runApp()
runApp()
runApp()
runApp()
View(all_text)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
library(shiny)
library(ggplot2)
library(zip)
library(httr)
library(rvest)
library(tidytext)
library(wordcloud)
library(tidyverse)
library(rtweet)
library(igraph)
#scraping_wiki <- read_html("https://weheartit.com/")
#scraping_wiki <- read_html("https://weheartit.com/articles")
#scraping_wiki <- read_html("https://weheartit.com/podcasts")
scraping_wiki <- read_html("https://www.nytimes.com/")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(zip)
zip("Zip/zasas.zip", c("Zip", "Zip"))
shiny::runApp()
unlink("Zip")
unlink("Zip", recursive = TRUE)
unlink("/Zip", recursive = TRUE)
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
